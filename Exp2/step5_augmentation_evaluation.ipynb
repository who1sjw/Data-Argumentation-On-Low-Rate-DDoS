{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e824acf",
   "metadata": {},
   "source": [
    "# Step 5: Data Augmentation Evaluation for Slowloris vs. Benign\n",
    "\n",
    "Four experiments using different class balances with a 70/30 split, evaluating five classifiers and four augmentation methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Placeholder augmentation functions\n",
    "def augment_cvae(X, y, n_samples):\n",
    "    return X, y  # TODO: CVAE implementation\n",
    "\n",
    "def augment_gan(X, y, n_samples):\n",
    "    return X, y  # TODO: GAN implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb829627",
   "metadata": {},
   "source": [
    "## Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = pd.read_csv('split_by_label/BenignTraffic.csv')\n",
    "slow = pd.read_csv('split_by_label/DDoS-SlowLoris.csv')\n",
    "print(f'Benign samples: {len(benign)}, Slowloris samples: {len(slow)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3188349",
   "metadata": {},
   "source": [
    "## Define Augmentation & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a045231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, method, target_size=None):\n",
    "    if method == 'SMOTE': return SMOTE(random_state=42).fit_resample(X, y)\n",
    "    if method == 'ADASYN': return ADASYN(random_state=42).fit_resample(X, y)\n",
    "    if method == 'CVAE':   return augment_cvae(X, y, n_samples=target_size)\n",
    "    if method == 'GAN':    return augment_gan(X, y, n_samples=target_size)\n",
    "    return X, y\n",
    "\n",
    "def evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'MLP': MLPClassifier(random_state=42, max_iter=500),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=500, random_state=42),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'ROC-AUC': roc_auc_score(y_test, y_proba)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ede30",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b72300",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {\n",
    "    'Test1': {'benign': 23426,   'slow': 23426},\n",
    "    'Test2': {'benign': 1098195, 'slow': 23426},\n",
    "    'Test3': {'benign': 23426//2, 'slow': 23426//2},\n",
    "    'Test4': {'benign': 23426*2,  'slow': 23426}\n",
    "}\n",
    "all_results = []\n",
    "for test_name, sizes in tests.items():\n",
    "    replace_b = sizes['benign'] > len(benign)\n",
    "    replace_s = sizes['slow']   > len(slow)\n",
    "    df_b = benign.sample(n=sizes['benign'], random_state=42, replace=replace_b)\n",
    "    df_s = slow.sample(n=sizes['slow'], random_state=42, replace=replace_s)\n",
    "    df = pd.concat([df_b, df_s], ignore_index=True)\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = (df['label']=='DDoS-SlowLoris').astype(int)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    for method in ['None','SMOTE','ADASYN','CVAE','GAN']:\n",
    "        X_aug, y_aug = augment_data(X_tr, y_tr, method, target_size=sizes['benign'])\n",
    "        df_res = evaluate_models(X_aug, y_aug, X_te, y_te)\n",
    "        df_res['Test'] = test_name\n",
    "        df_res['Augmentation'] = method\n",
    "        all_results.append(df_res)\n",
    "results_df = pd.concat(all_results, ignore_index=True)\n",
    "print('### Combined Results ###')\n",
    "display(results_df)\n",
    "results_df.to_csv('augmentation_evaluation_results_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb4d6c",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=results_df, x='Test', y='F1-Score', hue='Augmentation', palette='plasma')\n",
    "plt.title('F1-Score by Test and Augmentation Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23da8d6",
   "metadata": {},
   "source": [
    "**Conclusion:** Four tests with corrected sizes and sampling logic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
