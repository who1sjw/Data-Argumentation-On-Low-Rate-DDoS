{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c0e152",
   "metadata": {},
   "source": [
    "## Load and clean data, define test scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_split import split_features, get_feature_indices\n",
    "\n",
    "# 1) Read all CSV files\n",
    "csv_dir = 'CIC-IDS-2017'\n",
    "df_list = []\n",
    "for fname in os.listdir(csv_dir):\n",
    "    if fname.endswith('.csv'):\n",
    "        df_list.append(pd.read_csv(os.path.join(csv_dir, fname)))\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# **New**: Strip whitespace from column names to ensure 'Label' can be located\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 2) Basic cleaning: Replace infinite values, drop missing values\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 3) Split dataset into subsets based on label\n",
    "df_benign       = df[df['Label'] == 'BENIGN']\n",
    "df_slowloris    = df[df['Label'] == 'DoS slowloris']\n",
    "df_slowhttptest = df[df['Label'] == 'DoS Slowhttptest']\n",
    "\n",
    "# 4) Feature type split: continuous vs. discrete\n",
    "feature_df = df.drop('Label', axis=1)\n",
    "cont_cols, disc_cols = split_features(feature_df)\n",
    "cont_idx, disc_idx   = get_feature_indices(feature_df, cont_cols, disc_cols)\n",
    "\n",
    "# 5) Print check\n",
    "print(f\"Continuous features ({len(cont_cols)}):\")\n",
    "for c in cont_cols:\n",
    "    print(f\"  - {c}\")\n",
    "\n",
    "print(f\"\\nDiscrete features ({len(disc_cols)}):\")\n",
    "for c in disc_cols:\n",
    "    print(f\"  - {c}\")\n",
    "\n",
    "# 6) Define four test scenarios\n",
    "tests = {\n",
    "    'Test1': {\n",
    "        'benign':       len(df_slowloris),\n",
    "        'slowloris':    len(df_slowloris),\n",
    "        'slowhttptest': len(df_slowloris),\n",
    "    },\n",
    "    'Test2': {\n",
    "        'benign':       len(df_benign),\n",
    "        'slowloris':    len(df_slowloris),\n",
    "        'slowhttptest': len(df_slowhttptest),\n",
    "    },\n",
    "    'Test3': {\n",
    "        'benign':       len(df_slowloris) // 2,\n",
    "        'slowloris':    len(df_slowloris) // 2,\n",
    "        'slowhttptest': len(df_slowhttptest) // 2,\n",
    "    },\n",
    "    'Test4': {\n",
    "        'benign':       len(df_slowloris) * 2,\n",
    "        'slowloris':    len(df_slowloris),\n",
    "        'slowhttptest': len(df_slowhttptest),\n",
    "    },\n",
    "}\n",
    "\n",
    "# 7) Print scenario check\n",
    "for name, sizes in tests.items():\n",
    "    print(f\"{name}: BENIGN={sizes['benign']}, Slowloris={sizes['slowloris']}, Slowhttptest={sizes['slowhttptest']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b24ae",
   "metadata": {},
   "source": [
    "## Define augmentation functions supporting CVAE/GAN/SMOTE/ADASYN methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from augment_module import augment_cvae, augment_gan\n",
    "\n",
    "def apply_augmentation(\n",
    "    X_train, y_train,\n",
    "    method,\n",
    "    cont_idx, disc_idx,\n",
    "    imbalance_thresh: float = 0.10,\n",
    "    outlier_thresh: float = 3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Supported methods: 'None','SMOTENC','CVAE','GAN'\n",
    "      - Only augment when the minority class proportion < imbalance_thresh (default 0.10)\n",
    "      - SMOTENC: Over-sample both continuous and categorical features\n",
    "      - CVAE/GAN: Apply outlier filtering after generation, and synchronize filtering of y_gen\n",
    "      - If X_gen is empty, skip augmentation\n",
    "    \"\"\"\n",
    "    # 1) imbalance check\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    if method == 'None' or counts.min() / counts.sum() >= imbalance_thresh:\n",
    "        return X_train, y_train\n",
    "\n",
    "    # 2) Augmentation branch\n",
    "    if method == 'CVAE':\n",
    "        X_gen, y_gen = augment_cvae(X_train, y_train)\n",
    "    elif method == 'GAN':\n",
    "        X_gen, y_gen = augment_gan(X_train, y_train)\n",
    "    elif method == 'SMOTENC':\n",
    "        sampler = SMOTENC(\n",
    "            categorical_features=disc_idx,\n",
    "            sampling_strategy='minority',\n",
    "            random_state=seed\n",
    "        )\n",
    "        X_res_full, y_res_full = sampler.fit_resample(X_train, y_train)\n",
    "        n_new = len(y_res_full) - len(y_train)\n",
    "        X_gen = X_res_full[-n_new:]\n",
    "        y_gen = y_res_full[-n_new:]\n",
    "    else:\n",
    "        return X_train, y_train\n",
    "\n",
    "    # If no new samples are generated, skip augmentation\n",
    "    if X_gen.shape[0] == 0:\n",
    "        return X_train, y_train\n",
    "\n",
    "    # 3) Quality filtering: Apply mean imputation to X_train and X_gen, then compute Mahalanobis distance\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    X_train_imp = imp.fit_transform(X_train)\n",
    "    X_gen_imp   = imp.transform(X_gen)\n",
    "\n",
    "    cov = EmpiricalCovariance().fit(X_train_imp)\n",
    "    md = cov.mahalanobis(X_gen_imp)\n",
    "    mask = md < outlier_thresh\n",
    "    X_gen = X_gen[mask]\n",
    "    y_gen = y_gen[mask]\n",
    "\n",
    "    # If no samples remain after filtering, skip augmentation\n",
    "    if X_gen.shape[0] == 0:\n",
    "        return X_train, y_train\n",
    "\n",
    "    # 4) Merge original and augmented data\n",
    "    X_res = np.vstack([X_train, X_gen])\n",
    "    y_res = np.concatenate([y_train, y_gen])\n",
    "\n",
    "    return X_res, y_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ed81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Disable TensorFlow / absl logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "\n",
    "# Create 'results' folder\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Aggregated results for all seeds\n",
    "all_confusions = []\n",
    "all_classreports = []\n",
    "\n",
    "# List of seeds\n",
    "seeds = [\n",
    "    1674292617, 1362507843, 1523173961, 3675946018, 1730465472,\n",
    "    2092905740, 3127331292, 233407511, 476689606, 638637055\n",
    "]\n",
    "\n",
    "# Epoch configuration\n",
    "epoch_range = [1, 2, 3, 5, 10, 15, 20, 25, 30, 50, 100]\n",
    "methods = ['None', 'SMOTENC', 'CVAE', 'GAN']\n",
    "\n",
    "count_key = {\n",
    "    'BENIGN':           'BenignN',\n",
    "    'DoS slowloris':    'SlowlorisN',\n",
    "    'DoS Slowhttptest': 'SlowhttptestN'\n",
    "}\n",
    "\n",
    "# Use Test2 only\n",
    "tests = {'Test2': tests['Test2']}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n>>> Running with seed = {seed}\")\n",
    "    \n",
    "    # Build model dictionary, including current seed in model name\n",
    "    models = {}\n",
    "    for ep in epoch_range:\n",
    "        models[f'MLP-{ep}ep-s{seed}'] = MLPClassifier(\n",
    "            hidden_layer_sizes=(32,),\n",
    "            max_iter=ep,\n",
    "            tol=1e-3,\n",
    "            random_state=seed,\n",
    "            early_stopping=False\n",
    "        )\n",
    "        models[f'LogReg-{ep}ep-s{seed}'] = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                max_iter=ep,\n",
    "                tol=1e-3,\n",
    "                class_weight='balanced',\n",
    "                random_state=seed,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        )\n",
    "        models[f'RF-{ep}ep-s{seed}'] = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        models[f'XGB-{ep}ep-s{seed}'] = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    for test_name, sizes in tests.items():\n",
    "        print(f\"\\n=== Scenario: {test_name} ===\")\n",
    "\n",
    "        keep_benign = min(sizes['benign'], 50000)\n",
    "        b  = df_benign.sample(n=keep_benign, random_state=seed, replace=(keep_benign > len(df_benign)))\n",
    "        s1 = df_slowloris.sample(n=sizes['slowloris'], random_state=seed, replace=(sizes['slowloris'] > len(df_slowloris)))\n",
    "        s2 = df_slowhttptest.sample(n=sizes['slowhttptest'], random_state=seed, replace=(sizes['slowhttptest'] > len(df_slowhttptest)))\n",
    "\n",
    "        sample_counts = {\n",
    "            'Scenario':       test_name,\n",
    "            'BenignN':        len(b),\n",
    "            'SlowlorisN':     len(s1),\n",
    "            'SlowhttptestN':  len(s2)\n",
    "        }\n",
    "        print(\"Sample counts â†’\", sample_counts)\n",
    "\n",
    "        data = pd.concat([b, s1, s2]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        X, y = data.drop('Label', axis=1).values, data['Label'].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "\n",
    "        le = LabelEncoder().fit(y_train)\n",
    "        y_train_enc, y_test_enc = le.transform(y_train), le.transform(y_test)\n",
    "\n",
    "        for method in methods:\n",
    "            X_res, y_res = apply_augmentation(X_train, y_train_enc, method, cont_idx, disc_idx)\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            X_res = imputer.fit_transform(X_res)\n",
    "            X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "            for model_name, clf in models.items():\n",
    "                clf.fit(X_res, y_res)\n",
    "                y_pred_enc = clf.predict(X_test_imp)\n",
    "                y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "                cm = confusion_matrix(y_test, y_pred, labels=le.classes_)\n",
    "                for i, actual in enumerate(le.classes_):\n",
    "                    row = {\n",
    "                        'Seed': seed,\n",
    "                        'Scenario': test_name,\n",
    "                        'Augmentation': method,\n",
    "                        'Model': model_name,\n",
    "                        'ActualClass': actual\n",
    "                    }\n",
    "                    for j, pred in enumerate(le.classes_):\n",
    "                        row[f'Pred_{pred}'] = cm[i, j]\n",
    "                    all_confusions.append(row)\n",
    "\n",
    "                cr_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "                cr_df = pd.DataFrame(cr_dict).transpose()\n",
    "                for cls in le.classes_:\n",
    "                    metrics = cr_df.loc[cls]\n",
    "                    row = {\n",
    "                        'Seed': seed,\n",
    "                        'Scenario': test_name,\n",
    "                        'Augmentation': method,\n",
    "                        'Model': model_name,\n",
    "                        'Class': cls,\n",
    "                        'Precision': metrics['precision'],\n",
    "                        'Recall': metrics['recall'],\n",
    "                        'F1-Score': metrics['f1-score'],\n",
    "                        'Support': metrics['support'],\n",
    "                        'TotalSamples': sample_counts[count_key[cls]]\n",
    "                    }\n",
    "                    all_classreports.append(row)\n",
    "\n",
    "# Save results to CSV files\n",
    "pd.DataFrame(all_confusions).to_csv('results/confusion_matrices_all_seeds.csv', index=False)\n",
    "pd.DataFrame(all_classreports).to_csv('results/classification_reports_all_seeds.csv', index=False)\n",
    "\n",
    "print(\"\\n All seed results saved to:\")\n",
    "print(\" - results/confusion_matrices_all_seeds.csv\")\n",
    "print(\" - results/classification_reports_all_seeds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Explicitly retain the string \"None\" (to avoid being interpreted as NaN)\n",
    "report_df = pd.read_csv('results/classification_reports_all_seeds.csv', keep_default_na=False)\n",
    "\n",
    "# Keep only three classes\n",
    "target_classes = ['BENIGN', 'DoS slowloris', 'DoS Slowhttptest']\n",
    "report_df = report_df[report_df['Class'].isin(target_classes)]\n",
    "\n",
    "# Strip seed suffix from model name\n",
    "def strip_seed(model_name):\n",
    "    return re.sub(r'-s\\d+$', '-avg', model_name)\n",
    "\n",
    "report_df['ModelConfig'] = report_df['Model'].apply(strip_seed)\n",
    "\n",
    "# Group by Scenario + Augmentation + Model + Class\n",
    "avg_report = report_df.groupby(\n",
    "    ['Scenario', 'Augmentation', 'ModelConfig', 'Class']\n",
    ")[['Precision', 'Recall', 'F1-Score', 'Support', 'TotalSamples']].mean().reset_index()\n",
    "\n",
    "# Rename column for consistent output format\n",
    "avg_report = avg_report.rename(columns={'ModelConfig': 'Model'})\n",
    "\n",
    "print(\"Per-Class Average Report Across Seeds (with 'None'):\")\n",
    "display(avg_report)\n",
    "\n",
    "# Calculate F1-score\n",
    "macro_f1 = avg_report.groupby(['Model', 'Augmentation'])[['F1-Score']].mean().rename(\n",
    "    columns={'F1-Score': 'Macro-F1'}\n",
    ").reset_index()\n",
    "\n",
    "print(\"Macro-Averaged F1-Score Across Classes:\")\n",
    "display(macro_f1)\n",
    "\n",
    "# Save results\n",
    "avg_report.to_csv(\"results/classification_report_perclass_avg.csv\", index=False)\n",
    "macro_f1.to_csv(\"results/classification_macro_f1_avg.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
