{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bd2b37",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing and Splitting by Type\n",
    "\n",
    "This notebook loads the full CICIoT2023 dataset, cleans the data, and splits it by attack type (the `label` column). The preprocessed dataset is also saved as **preprocessed_CICIoT2023.csv** for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d3e3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a2a28f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files found: 10\n",
      "Reading file: ./CICIoT2023/part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Reading file: ./CICIoT2023/part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "Combined dataset shape: (2366956, 47)\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset directory and pattern for CSV files\n",
    "dataset_dir = \"./CICIoT2023\"\n",
    "file_pattern = os.path.join(dataset_dir, \"part*.csv\")\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = glob.glob(file_pattern)\n",
    "print(\"Number of CSV files found:\", len(csv_files))\n",
    "\n",
    "# Load and concatenate all CSV files into a single DataFrame\n",
    "data_list = []\n",
    "for file in csv_files:\n",
    "    print(\"Reading file:\", file)\n",
    "    df = pd.read_csv(file)\n",
    "    data_list.append(df)\n",
    "\n",
    "data = pd.concat(data_list, ignore_index=True)\n",
    "print(\"Combined dataset shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7999d2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " flow_duration      0\n",
      "Header_Length      0\n",
      "Protocol Type      0\n",
      "Duration           0\n",
      "Rate               0\n",
      "Srate              0\n",
      "Drate              0\n",
      "fin_flag_number    0\n",
      "syn_flag_number    0\n",
      "rst_flag_number    0\n",
      "psh_flag_number    0\n",
      "ack_flag_number    0\n",
      "ece_flag_number    0\n",
      "cwr_flag_number    0\n",
      "ack_count          0\n",
      "syn_count          0\n",
      "fin_count          0\n",
      "urg_count          0\n",
      "rst_count          0\n",
      "HTTP               0\n",
      "HTTPS              0\n",
      "DNS                0\n",
      "Telnet             0\n",
      "SMTP               0\n",
      "SSH                0\n",
      "IRC                0\n",
      "TCP                0\n",
      "UDP                0\n",
      "DHCP               0\n",
      "ARP                0\n",
      "ICMP               0\n",
      "IPv                0\n",
      "LLC                0\n",
      "Tot sum            0\n",
      "Min                0\n",
      "Max                0\n",
      "AVG                0\n",
      "Std                0\n",
      "Tot size           0\n",
      "IAT                0\n",
      "Number             0\n",
      "Magnitue           0\n",
      "Radius             0\n",
      "Covariance         0\n",
      "Variance           0\n",
      "Weight             0\n",
      "label              0\n",
      "dtype: int64\n",
      "Dataset shape after dropping missing values: (2366956, 47)\n",
      "Dataset shape after dropping duplicates: (2366956, 47)\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_counts = data.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_counts)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_cleaned = data.dropna().reset_index(drop=True)\n",
    "print(\"Dataset shape after dropping missing values:\", data_cleaned.shape)\n",
    "\n",
    "# Drop duplicate rows if any\n",
    "data_cleaned = data_cleaned.drop_duplicates().reset_index(drop=True)\n",
    "print(\"Dataset shape after dropping duplicates:\", data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71b4d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the cleaned dataset:\n",
      "   flow_duration  Header_Length  Protocol Type  Duration       Rate  \\\n",
      "0       4.625763           84.0           17.0      64.0   0.432361   \n",
      "1      76.135781       428611.8            8.2     151.9  42.352135   \n",
      "2       4.549627          108.0            6.0      64.0   0.762226   \n",
      "3       0.000000           54.0            6.0      64.0   0.000000   \n",
      "4       0.000000            0.0            1.0      64.0   3.178249   \n",
      "\n",
      "       Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  ...  \\\n",
      "0   0.432361    0.0              0.0              0.0              0.0  ...   \n",
      "1  42.352135    0.0              0.0              0.0              0.0  ...   \n",
      "2   0.762226    0.0              0.0              1.0              0.0  ...   \n",
      "3   0.000000    0.0              0.0              1.0              0.0  ...   \n",
      "4   3.178249    0.0              0.0              0.0              0.0  ...   \n",
      "\n",
      "           Std  Tot size           IAT  Number   Magnitue       Radius  \\\n",
      "0     0.000000      42.0  8.300688e+07     9.5   9.165151     0.000000   \n",
      "1  1707.500922     655.0  1.665202e+08    13.5  41.470705  2419.498399   \n",
      "2     0.000000      54.0  8.336142e+07     9.5  10.392305     0.000000   \n",
      "3     0.000000      54.0  8.308992e+07     9.5  10.392305     0.000000   \n",
      "4     0.000000      42.0  8.313213e+07     9.5   9.165151     0.000000   \n",
      "\n",
      "     Covariance  Variance  Weight                    label  \n",
      "0  0.000000e+00       0.0  141.55            DoS-UDP_Flood  \n",
      "1  2.944407e+06       1.0  244.60            BenignTraffic  \n",
      "2  0.000000e+00       0.0  141.55  DDoS-SynonymousIP_Flood  \n",
      "3  0.000000e+00       0.0  141.55           DDoS-SYN_Flood  \n",
      "4  0.000000e+00       0.0  141.55          DDoS-ICMP_Flood  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows of the cleaned dataset to verify the preprocessing steps\n",
    "print(\"First 5 rows of the cleaned dataset:\")\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "new_save_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved as preprocessed_CICIoT2023.csv. Shape: (2366956, 47)\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed dataset for future use\n",
    "preprocessed_file = \"preprocessed_CICIoT2023.csv\"\n",
    "data_cleaned.to_csv(preprocessed_file, index=False)\n",
    "print(f\"Preprocessed dataset saved as {preprocessed_file}. Shape: {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9e50d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset: ['DoS-UDP_Flood' 'BenignTraffic' 'DDoS-SynonymousIP_Flood'\n",
      " 'DDoS-SYN_Flood' 'DDoS-ICMP_Flood' 'DDoS-RSTFINFlood' 'DDoS-UDP_Flood'\n",
      " 'DDoS-ACK_Fragmentation' 'Mirai-udpplain' 'DDoS-UDP_Fragmentation'\n",
      " 'DDoS-TCP_Flood' 'DoS-TCP_Flood' 'Mirai-greeth_flood' 'DDoS-PSHACK_Flood'\n",
      " 'Mirai-greip_flood' 'DoS-SYN_Flood' 'MITM-ArpSpoofing'\n",
      " 'Recon-HostDiscovery' 'DDoS-ICMP_Fragmentation' 'VulnerabilityScan'\n",
      " 'Recon-PortScan' 'DoS-HTTP_Flood' 'Uploading_Attack' 'DNS_Spoofing' 'XSS'\n",
      " 'Backdoor_Malware' 'Recon-OSScan' 'DDoS-HTTP_Flood' 'BrowserHijacking'\n",
      " 'DDoS-SlowLoris' 'Recon-PingSweep' 'SqlInjection' 'DictionaryBruteForce'\n",
      " 'CommandInjection']\n",
      "Label: DoS-UDP_Flood, Number of samples: 168753\n",
      "Label: BenignTraffic, Number of samples: 55859\n",
      "Label: DDoS-SynonymousIP_Flood, Number of samples: 182094\n",
      "Label: DDoS-SYN_Flood, Number of samples: 206146\n",
      "Label: DDoS-ICMP_Flood, Number of samples: 364557\n",
      "Label: DDoS-RSTFINFlood, Number of samples: 204892\n",
      "Label: DDoS-UDP_Flood, Number of samples: 274432\n",
      "Label: DDoS-ACK_Fragmentation, Number of samples: 14498\n",
      "Label: Mirai-udpplain, Number of samples: 45219\n",
      "Label: DDoS-UDP_Fragmentation, Number of samples: 14611\n",
      "Label: DDoS-TCP_Flood, Number of samples: 228873\n",
      "Label: DoS-TCP_Flood, Number of samples: 135180\n",
      "Label: Mirai-greeth_flood, Number of samples: 49844\n",
      "Label: DDoS-PSHACK_Flood, Number of samples: 207971\n",
      "Label: Mirai-greip_flood, Number of samples: 38157\n",
      "Label: DoS-SYN_Flood, Number of samples: 102049\n",
      "Label: MITM-ArpSpoofing, Number of samples: 15618\n",
      "Label: Recon-HostDiscovery, Number of samples: 6761\n",
      "Label: DDoS-ICMP_Fragmentation, Number of samples: 22890\n",
      "Label: VulnerabilityScan, Number of samples: 1879\n",
      "Label: Recon-PortScan, Number of samples: 4198\n",
      "Label: DoS-HTTP_Flood, Number of samples: 3701\n",
      "Label: Uploading_Attack, Number of samples: 71\n",
      "Label: DNS_Spoofing, Number of samples: 9142\n",
      "Label: XSS, Number of samples: 160\n",
      "Label: Backdoor_Malware, Number of samples: 169\n",
      "Label: Recon-OSScan, Number of samples: 5046\n",
      "Label: DDoS-HTTP_Flood, Number of samples: 1467\n",
      "Label: BrowserHijacking, Number of samples: 295\n",
      "Label: DDoS-SlowLoris, Number of samples: 1176\n",
      "Label: Recon-PingSweep, Number of samples: 101\n",
      "Label: SqlInjection, Number of samples: 248\n",
      "Label: DictionaryBruteForce, Number of samples: 656\n",
      "Label: CommandInjection, Number of samples: 243\n"
     ]
    }
   ],
   "source": [
    "# Split Data by Type based on the 'label' column\n",
    "\n",
    "# Get unique labels in the dataset\n",
    "unique_labels = data_cleaned['label'].unique()\n",
    "print(\"Unique labels in the dataset:\", unique_labels)\n",
    "\n",
    "# Create a dictionary to store data subsets by label\n",
    "data_by_label = {label: data_cleaned[data_cleaned['label'] == label] for label in unique_labels}\n",
    "\n",
    "# Print the number of samples for each label\n",
    "for label, df in data_by_label.items():\n",
    "    print(f\"Label: {label}, Number of samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8e8a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DoS-UDP_Flood data to ./split_by_label/DoS-UDP_Flood.csv\n",
      "Saved BenignTraffic data to ./split_by_label/BenignTraffic.csv\n",
      "Saved DDoS-SynonymousIP_Flood data to ./split_by_label/DDoS-SynonymousIP_Flood.csv\n",
      "Saved DDoS-SYN_Flood data to ./split_by_label/DDoS-SYN_Flood.csv\n",
      "Saved DDoS-ICMP_Flood data to ./split_by_label/DDoS-ICMP_Flood.csv\n",
      "Saved DDoS-RSTFINFlood data to ./split_by_label/DDoS-RSTFINFlood.csv\n",
      "Saved DDoS-UDP_Flood data to ./split_by_label/DDoS-UDP_Flood.csv\n",
      "Saved DDoS-ACK_Fragmentation data to ./split_by_label/DDoS-ACK_Fragmentation.csv\n",
      "Saved Mirai-udpplain data to ./split_by_label/Mirai-udpplain.csv\n",
      "Saved DDoS-UDP_Fragmentation data to ./split_by_label/DDoS-UDP_Fragmentation.csv\n",
      "Saved DDoS-TCP_Flood data to ./split_by_label/DDoS-TCP_Flood.csv\n",
      "Saved DoS-TCP_Flood data to ./split_by_label/DoS-TCP_Flood.csv\n",
      "Saved Mirai-greeth_flood data to ./split_by_label/Mirai-greeth_flood.csv\n",
      "Saved DDoS-PSHACK_Flood data to ./split_by_label/DDoS-PSHACK_Flood.csv\n",
      "Saved Mirai-greip_flood data to ./split_by_label/Mirai-greip_flood.csv\n",
      "Saved DoS-SYN_Flood data to ./split_by_label/DoS-SYN_Flood.csv\n",
      "Saved MITM-ArpSpoofing data to ./split_by_label/MITM-ArpSpoofing.csv\n",
      "Saved Recon-HostDiscovery data to ./split_by_label/Recon-HostDiscovery.csv\n",
      "Saved DDoS-ICMP_Fragmentation data to ./split_by_label/DDoS-ICMP_Fragmentation.csv\n",
      "Saved VulnerabilityScan data to ./split_by_label/VulnerabilityScan.csv\n",
      "Saved Recon-PortScan data to ./split_by_label/Recon-PortScan.csv\n",
      "Saved DoS-HTTP_Flood data to ./split_by_label/DoS-HTTP_Flood.csv\n",
      "Saved Uploading_Attack data to ./split_by_label/Uploading_Attack.csv\n",
      "Saved DNS_Spoofing data to ./split_by_label/DNS_Spoofing.csv\n",
      "Saved XSS data to ./split_by_label/XSS.csv\n",
      "Saved Backdoor_Malware data to ./split_by_label/Backdoor_Malware.csv\n",
      "Saved Recon-OSScan data to ./split_by_label/Recon-OSScan.csv\n",
      "Saved DDoS-HTTP_Flood data to ./split_by_label/DDoS-HTTP_Flood.csv\n",
      "Saved BrowserHijacking data to ./split_by_label/BrowserHijacking.csv\n",
      "Saved DDoS-SlowLoris data to ./split_by_label/DDoS-SlowLoris.csv\n",
      "Saved Recon-PingSweep data to ./split_by_label/Recon-PingSweep.csv\n",
      "Saved SqlInjection data to ./split_by_label/SqlInjection.csv\n",
      "Saved DictionaryBruteForce data to ./split_by_label/DictionaryBruteForce.csv\n",
      "Saved CommandInjection data to ./split_by_label/CommandInjection.csv\n"
     ]
    }
   ],
   "source": [
    "# Optionally, save each subset to a separate CSV file\n",
    "output_dir = \"./split_by_label\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for label, df in data_by_label.items():\n",
    "    # Create a safe filename by replacing problematic characters\n",
    "    safe_label = label.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "    output_file = os.path.join(output_dir, f\"{safe_label}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {label} data to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
